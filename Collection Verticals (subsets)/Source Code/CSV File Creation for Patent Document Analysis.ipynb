{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f100f3f",
   "metadata": {},
   "source": [
    "*Licensed under the MIT License. See LICENSE-CODE in the repository root for details.*\n",
    "\n",
    "*Copyright (c) 2025 Eleni Kamateri*\n",
    "\n",
    "### Parsing and CSV File Generation for WPI Analysis\n",
    "\n",
    "This script generates a CSV file containing essential data for analyzing patent documents in a core vertical of the WPI dataset. The file includes key information such as:\n",
    " \n",
    "1. Document ucid\n",
    "2. Document date\n",
    "3. Classification Labels\n",
    "4. The presence of abstract, description, and claims\n",
    "\n",
    "#### Requirements\n",
    "\n",
    "1. This script should be applied to the extracted patent documents, which are organized into separate folders. It requires that the script \"*7z Files Extraction and Organization by Vertical.ipynb*\" has been run first.\n",
    "\n",
    "#### Label Formatting\n",
    "\n",
    "Labels for each document are concatenated into a single string, separated by commas. To avoid conflicts with the default CSV delimiter, we use a semicolon (;) separator when storing the data.\n",
    "\n",
    "#### Configurable Parameters\n",
    "\n",
    "Researchers can customize the extraction process using the following parameters:\n",
    "\n",
    "**vertical_origin_path** – Path to the core vertical of the WPI dataset, containing the extracted files to be parsed for CSV creation. \n",
    "\n",
    "        Example: \"/YOUR_PATH/WPI-Dataset/EP/\".  \n",
    "\n",
    "**csv_file_name** – The name of the CSV file that will be generated\n",
    "        \n",
    "        Example: \"csv_file_for_wpi_analysis\".\n",
    "\n",
    "**main_further** – Include main and further classification labels?\n",
    "        \n",
    "        0: No\n",
    "        1: Yes\n",
    "\n",
    "\n",
    "**ipcr** – Include IPCR classification labels?\n",
    "        \n",
    "        0: No\n",
    "        1: Yes\n",
    "    \n",
    "**cpc** – Include CPC classification labels?\n",
    "        \n",
    "        0: No\n",
    "        1: Yes\n",
    "        \n",
    "\n",
    "**vertical** – Select dataset vertical:\n",
    "       \n",
    "        0: EP\n",
    "        1: WO\n",
    "        2: US\n",
    "        3: CN\n",
    "        4: JP\n",
    "        5: KR\n",
    "        \n",
    "#### Processing Time Estimates\n",
    "\n",
    "    EP vertical: ~58,777.60 sec\n",
    "    WO vertical: ~42,645.45 sec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ff4a17",
   "metadata": {},
   "source": [
    "### Import all required libraries for the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e84a4a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04661c2",
   "metadata": {},
   "source": [
    "### Set the required parameters for the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22baab87",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertical_origin_path=\"/YOUR_PATH/WPI-Dataset/EP/\"\n",
    "csv_file_name=\"csv_file_for_wpi_analysis\"\n",
    "main_further=0\n",
    "ipcr=1\n",
    "cpc=0\n",
    "vertical=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80f6f6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if vertical==0:\n",
    "    csv_file_name=\"EP\"+\"_\"+csv_file_name\n",
    "elif vertical==1:\n",
    "    csv_file_name=\"WO\"+\"_\"+csv_file_name\n",
    "elif vertical==1:\n",
    "    csv_file_name=\"US\"+\"_\"+csv_file_name\n",
    "elif vertical==1:\n",
    "    csv_file_name=\"CN\"+\"_\"+csv_file_name\n",
    "elif vertical==1:\n",
    "    csv_file_name=\"JP\"+\"_\"+csv_file_name\n",
    "elif vertical==1:\n",
    "    csv_file_name=\"KR\"+\"_\"+csv_file_name\n",
    "else:\n",
    "    print(\"Provide a valid vertical number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e91b7f",
   "metadata": {},
   "source": [
    "### Parse the data and generate the CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d681ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the time\n",
    "start_time = time.time()\n",
    "\n",
    "df_class = pd.DataFrame()\n",
    "counter_class=0\n",
    "\n",
    "for folder_level_1 in os.listdir(vertical_origin_path): #CC\n",
    "    for folder_level_2 in os.listdir(vertical_origin_path+\"/\"+folder_level_1): #nnnnnn\n",
    "        for folder_level_3 in os.listdir(vertical_origin_path+\"/\"+folder_level_1+\"/\"+folder_level_2): #nn\n",
    "            for folder_level_4 in os.listdir(vertical_origin_path+\"/\"+folder_level_1+\"/\"+folder_level_2+\"/\"+folder_level_3): #nn\n",
    "                for folder_level_5 in os.listdir(vertical_origin_path+\"/\"+folder_level_1+\"/\"+folder_level_2+\"/\"+folder_level_3+\"/\"+folder_level_4): #nn                                        \n",
    "                    for folder_level_6 in os.listdir(vertical_origin_path+\"/\"+folder_level_1+\"/\"+folder_level_2+\"/\"+folder_level_3+\"/\"+folder_level_4+\"/\"+folder_level_5): #nn                                        \n",
    "                        for files in os.listdir(vertical_origin_path+\"/\"+folder_level_1+\"/\"+folder_level_2+\"/\"+folder_level_3+\"/\"+folder_level_4+\"/\"+folder_level_5+\"/\"+folder_level_6): #nn                                        \n",
    "\n",
    "                            counter_class=counter_class+1\n",
    "                            if counter_class%100000==0:\n",
    "                                print(counter_class)\n",
    "\n",
    "                            content = open(vertical_origin_path+\"/\"+folder_level_1+\"/\"+folder_level_2+\"/\"+folder_level_3+\"/\"+folder_level_4+\"/\"+folder_level_5+\"/\"+folder_level_6+\"/\"+files,'r',encoding='utf-8').read()\n",
    "                            soup = BeautifulSoup(content, 'xml')                           \n",
    "                            document_info = soup.find_all(\"patent-document\")                                  \n",
    "\n",
    "                            try:\n",
    "                                ucid=document_info[0]['ucid']\n",
    "                            except Exception:\n",
    "                                ucid=''\n",
    "                                print(\"Exception 1, ucid does not exist\", files)\n",
    "\n",
    "                            try:\n",
    "                                date=document_info[0]['date']\n",
    "                            except Exception:\n",
    "                                date=''\n",
    "                                print(\"Exception 2, date does not exist\", files)\n",
    "                            \n",
    "                            main_code=''\n",
    "                            further_codes_help=[]\n",
    "                            further_codes_list=[]\n",
    "                            ipcr_codes_help=[]\n",
    "                            ipcr_codes_list=[]\n",
    "                            cpc_codes_help=[]\n",
    "                            cpc_codes_list=[]\n",
    "                            \n",
    "                            if main_further == 1:\n",
    "                                for main_classification in soup.find_all('main-classification'):\n",
    "                                    main_code=main_classification.getText()\n",
    "\n",
    "                                for further_classification in soup.find_all('further-classification'):\n",
    "                                    further_code=further_classification.getText()\n",
    "                                    further_codes_help.append(further_code) if further_code not in further_codes_help else further_codes_help\n",
    "                                further_codes_list = \", \".join(further_codes_help)                                        \n",
    "\n",
    "                            if ipcr == 1:\n",
    "                                for classification_ipcr in soup.find_all('classification-ipcr'):\n",
    "                                    ipcr_code=classification_ipcr.getText()\n",
    "                                    ipcr_codes_help.append(ipcr_code) if ipcr_code not in ipcr_codes_help else ipcr_codes_help\n",
    "                                ipcr_codes_list = \", \".join(ipcr_codes_help) \n",
    "\n",
    "                            if cpc == 1:\n",
    "                                for classification_cpc in soup.find_all('classification-cpc'):\n",
    "                                    cpc_code=classification_cpc.getText()\n",
    "                                    cpc_codes_help.append(cpc_code) if cpc_code not in cpc_codes_help else cpc_codes_help\n",
    "                                cpc_codes_list = \", \".join(cpc_codes_help)\n",
    "           \n",
    "                            abstract_en_exist=0        \n",
    "                            abstract_en=soup.find('abstract', attrs={'lang':'EN'})\n",
    "                            if abstract_en != None:\n",
    "                                abstract_en_exist=1\n",
    "\n",
    "                            description_en_exist=0\n",
    "                            description_en = soup.find('description', attrs={'lang':'EN'})\n",
    "                            if description_en != None:\n",
    "                                description_en_exist=1\n",
    "\n",
    "                            claims_en_exist=0\n",
    "                            claims_en = soup.find('claims', attrs={'lang':'EN'})\n",
    "                            if claims_en != None:\n",
    "                                claims_en_exist=1\n",
    "                                \n",
    "                                \n",
    "                            df_class.loc[counter_class-1, 'xml_file_name']=files\n",
    "                            df_class.loc[counter_class-1, 'ucid']=ucid\n",
    "                            df_class.loc[counter_class-1, 'date']=date\n",
    "                            if main_further == 1:\n",
    "                                df_class.loc[counter_class-1, 'main_classification']=main_code   \n",
    "                                df_class.loc[counter_class-1, 'further_classification']=further_codes_list \n",
    "                            if ipcr == 1:\n",
    "                                df_class.loc[counter_class-1, 'classification_ipcr']=ipcr_codes_list  \n",
    "                            if cpc == 1:\n",
    "                                df_class.loc[counter_class-1, 'classification_cpc']=cpc_codes_list  \n",
    "                            df_class.loc[counter_class-1, 'abstract_lang_en_exist']=abstract_en_exist\n",
    "                            df_class.loc[counter_class-1, 'description_lang_en_exist']=description_en_exist\n",
    "                            df_class.loc[counter_class-1, 'claims_lang_en_exist']=claims_en_exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1e4ddcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>xml_file_name</th>\n",
       "      <th>ucid</th>\n",
       "      <th>date</th>\n",
       "      <th>classification_ipcr</th>\n",
       "      <th>abstract_lang_en_exist</th>\n",
       "      <th>description_lang_en_exist</th>\n",
       "      <th>claims_lang_en_exist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EP-2677851-A1.xml</td>\n",
       "      <td>EP-2677851-A1</td>\n",
       "      <td>20140101</td>\n",
       "      <td>A01B  79/02        20060101AFI20120911BHEP    ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EP-2677852-A1.xml</td>\n",
       "      <td>EP-2677852-A1</td>\n",
       "      <td>20140101</td>\n",
       "      <td>A01D  43/063       20060101AFI20170131BHEP    ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EP-2677853-A1.xml</td>\n",
       "      <td>EP-2677853-A1</td>\n",
       "      <td>20140101</td>\n",
       "      <td>A01D  46/28        20060101AFI20120913BHEP    ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EP-2677854-A1.xml</td>\n",
       "      <td>EP-2677854-A1</td>\n",
       "      <td>20140101</td>\n",
       "      <td>E04D  11/00        20060101ALI20120913BHEP    ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EP-2677856-A1.xml</td>\n",
       "      <td>EP-2677856-A1</td>\n",
       "      <td>20140101</td>\n",
       "      <td>A01G  17/00        20060101AFI20120914BHEP    ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>EP-2677857-A1.xml</td>\n",
       "      <td>EP-2677857-A1</td>\n",
       "      <td>20140101</td>\n",
       "      <td>A01G  25/16        20060101AFI20140925BHEP    ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>EP-2677860-A1.xml</td>\n",
       "      <td>EP-2677860-A1</td>\n",
       "      <td>20140101</td>\n",
       "      <td>E03C   1/02        20060101ALI20120910BHEP    ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>EP-2677862-A1.xml</td>\n",
       "      <td>EP-2677862-A1</td>\n",
       "      <td>20140101</td>\n",
       "      <td>A01M  25/00        20060101AFI20120911BHEP    ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>EP-2677863-A1.xml</td>\n",
       "      <td>EP-2677863-A1</td>\n",
       "      <td>20140101</td>\n",
       "      <td>C12N   5/071       20100101ALI20120914BHEP    ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>EP-2677864-A1.xml</td>\n",
       "      <td>EP-2677864-A1</td>\n",
       "      <td>20140101</td>\n",
       "      <td>A01N  25/34        20060101ALI20160118BHEP    ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       xml_file_name           ucid      date  \\\n",
       "0  EP-2677851-A1.xml  EP-2677851-A1  20140101   \n",
       "1  EP-2677852-A1.xml  EP-2677852-A1  20140101   \n",
       "2  EP-2677853-A1.xml  EP-2677853-A1  20140101   \n",
       "3  EP-2677854-A1.xml  EP-2677854-A1  20140101   \n",
       "4  EP-2677856-A1.xml  EP-2677856-A1  20140101   \n",
       "5  EP-2677857-A1.xml  EP-2677857-A1  20140101   \n",
       "6  EP-2677860-A1.xml  EP-2677860-A1  20140101   \n",
       "7  EP-2677862-A1.xml  EP-2677862-A1  20140101   \n",
       "8  EP-2677863-A1.xml  EP-2677863-A1  20140101   \n",
       "9  EP-2677864-A1.xml  EP-2677864-A1  20140101   \n",
       "\n",
       "                                 classification_ipcr  abstract_lang_en_exist  \\\n",
       "0  A01B  79/02        20060101AFI20120911BHEP    ...                     1.0   \n",
       "1  A01D  43/063       20060101AFI20170131BHEP    ...                     1.0   \n",
       "2  A01D  46/28        20060101AFI20120913BHEP    ...                     1.0   \n",
       "3  E04D  11/00        20060101ALI20120913BHEP    ...                     1.0   \n",
       "4  A01G  17/00        20060101AFI20120914BHEP    ...                     1.0   \n",
       "5  A01G  25/16        20060101AFI20140925BHEP    ...                     1.0   \n",
       "6  E03C   1/02        20060101ALI20120910BHEP    ...                     1.0   \n",
       "7  A01M  25/00        20060101AFI20120911BHEP    ...                     1.0   \n",
       "8  C12N   5/071       20100101ALI20120914BHEP    ...                     1.0   \n",
       "9  A01N  25/34        20060101ALI20160118BHEP    ...                     1.0   \n",
       "\n",
       "   description_lang_en_exist  claims_lang_en_exist  \n",
       "0                        1.0                   1.0  \n",
       "1                        1.0                   1.0  \n",
       "2                        1.0                   1.0  \n",
       "3                        1.0                   1.0  \n",
       "4                        0.0                   0.0  \n",
       "5                        0.0                   0.0  \n",
       "6                        1.0                   1.0  \n",
       "7                        0.0                   0.0  \n",
       "8                        1.0                   1.0  \n",
       "9                        1.0                   1.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_class.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa7a25b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 7.6437201499938965 seconds ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4ee5d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_class.to_csv(vertical_origin_path+csv_file_name+\".csv\", sep =';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d122668",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
